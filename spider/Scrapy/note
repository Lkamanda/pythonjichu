# scrapy 
# 爬虫框架
     - scrapy 
     - pyspider 
     - crawley
- scrapy 框架介绍
    - 官网:https://doc.scrapy.org/en/latest/
    -  中文文档https://scrapy-chs.readthedocs.io/zh_CN/latest/index.html
- scrapy 概述
    - 包含各个部件
        - ScrapyEngine :神经中枢,核心
        - Scheduler调度器:  引擎发出来request请求,调度器需要处理,然后交换引擎
        - Downloader下载器:把引擎发来的requests发出请求,的到response
        - Spider爬虫    : 负责吧下载器得到的网页/结果进行分解,分解成数据+链接
        - ItemPipeline管道:详细处理Item
        
        - DownloadMiddleware下载中间件:自定义下载功能扩展组件
        - SpiderMiddleware爬虫中间件:对spider进行功能扩展
        
- 爬虫项目大概流程
    - 新建项目 ;scrapy     startproject xxxx
    - 明确需要目标/产出 : 编写item.py
    - 制作爬虫: 地址 sidler/xxxxspider.py
    - 存储内容 : pipelines.py

- ItemPipeline
    - 对应的是pipelines文件
    - 爬虫提取出数据库存入item后,item中保存的数据需要进一步处理,比如清洗,去重.存储
    - pipeline需处理process_item函数
        - process_item:
            - spider提取出来的item作为参数传入,同事传入的还有spider
            - 此方法必须实现
            - 必须返回一个Item对象,被丢弃的item不会被之后的pipeline
        - __init__ :构造函数
            - 进行一些必要的参数初始化
        - open_spider(spider)
            - spider对象被开启的时候调用
        - close_spider(spider):
            - 当spider对象被关闭的时候调用
- Spider
    - 对用的是Spiders下的文件
    - __init__:初始化爬虫名称,start_urls列表
    - start_requests:生成 Requests对象交给Scrapy下载返回response
    - parse:根据返回response解析出相应的item.item自动进入pipline:如果需要,解析出url.url自动交给
    requests模块,一直循环下去
    - start_request: 此方法仅能被调用一次,读取start_urls内容开启循环过程
        - name;设置爬虫名称
        - start_urls:设置开始第一次允许去的域名列表
        - allow_domains:spider允许去的域名列表
        - start_request(self): 只被调用一次
        - parse
        - log:日志介绍
        
- 中间件(DownloaderMiddlelewares)
    - 中间件事处于引擎和下载器之间的一层组件
    - 可以有很多个,可以按顺序加载执行
    - 作用是对发出的请求和返回的结果进行预处理
    - 在middlewares文件中
    - 需要在settings中设置以便生效
    - 编写中间件十分简单,中间件是scrapy.contirb.downloadermiddleware.DownloaderMiddleware的子类
    - 一般一个中间件完成一项功能
    - 必须实现以下一个或者多个方法
        - process_request(self, request, spider)
       
            - 在request 通过的时候被调用
            - 必须返回None或Response或Request或 raise IgnoreRequest
            - None:scrapy将继续处理该request
            - Request: scrapy 会停止调用process_request并重新返回request 
            - Response:scrapy 将不会调用其他的process_request或process_exception,
            直接将该response作为结果,同时会被调用process_response
        -  process_response(self, request, response, spider)
             
             - 跟proces_request 大同小异
             - 每次返回结果时候会自动调用
             - 可以有多个,按顺序调用
             
        - 案例代码 
                
                import random
                import base64
                # 从setting 试着文件中导入值
                from settings import USER_AGENTS
                from settings import PROXIES
                
                # 随机的USER-AGENT  
                class RandomUserAgent(object):
                    def process_request(self, request, spider):
                        useragent = random.chioce(USER_AGENTS)
                        request.headers.setdefault("User-Agent", useragent)
                 
                class RandomProxy(object):
                    def process_request(self,request, spider):
                        proxy = random.choice(PROXIES)
                        if proxy['user_passwd"] id None:
                            # 没有代理账户验证的代理使用方式
                            request.meta['proxy'] = "http://"+ proxy["ip_port]
                        else:
                            # 对账户密码进行base64编码转化
                            base64_userpassword = base64.b64encode(porxy['user_passwd'])
                            # 对应到代理服务器的命令格式里
                            request.headerd['Proxy-Authorization'] = 'Basic' + base64_userpasswd
                            request.meta['proxy'] = "http://" + proxy["ip_port"] 
        - settings
            USERA_AGENTS = [
            
            ]
            PROXIES = [
            ]
            
- 去重
    - 为了防止爬虫陷入死循环中,需要去重
    - 即在spider 中的parse函数中,返回Request的时候加上dont_filter =False参数
        - 添加过滤
            myspider(scrapy.Spider):
                def parse(....):
                    .....
                    
                    yield scrapy.Request(url=url, callback=self.parse, dont_filter =False)
        
- 如何在scrapy使用selenium
    - 可以放入中间件中的process_request函数中
    - 在函数中调用selenium,完成后返回Response
            
            class MyMiddleWare(object):
                def process_request(...):
                    
                    driver = webdriver.chrome()
                    html = driver.page_source
                    driver.quit()
                    
                    return HtmlResponse(url=request.url,encoding="utf-8",body=html, request=request)
- anaconda切换虚拟环境
    - windows : activate 环境名
    - linux: source activate 环境名     
- scrapy-shell:方便爬虫项的调试,测试xpath和css表达式,查看他们的工作方式,方便我们从网页中提取数据
   - scrapy shell "url"
   - view (response) : 使用浏览器打开输入的网址 
   - 启动后自动下载指定url网页
   - 下载完成后,url内容保存在response的变量中,如果需要,我们需要调用response
   - response
        - 爬取内容会保存在response中
        - response.body是网页的代码  
        - response.headers是返回http的头信息
        - response.xpath () 允许使用xpath语法选取内容
        - response.css() 允许使用css语法选取内容
   - selector 
        - 选择器,允许用户使用选择器来选择自己想要的内容
        - response.selector.xpath :response.xpath是 selector.xpath 的快捷方式
        - response.selector.css :response.css是他的快捷方式 
        - selector.extract: 把节点的内容用unicode形式返回 
        - selector.re:允许用户通过正则选取内容
   - 例子:
        divs = response.xpath("//div") 
        div[0]
        div[0].extract() 

# 分布式爬虫
- 单机爬虫问题
    - 单机效率
    - IO吞吐量
- 多爬虫问题
    -  数据共享
    -  在空间上不同的多态机器,可以成为分布式
- 需要做:
    - 共享队列
    - 去重
- Redis: 具有字典,集合,队列
    - 内存数据库
    - 可以落地到硬盘
    - 可以去重
    - 对保存的内容进行生命周期控制
    - 分布式爬虫scrapy + redis
        - 不使用scrapy队列使用redis
            - 需要在scrapy中settings设置队列为远程

- 内容保存数据库
    - Mongodb 非关系型数据库
    - Mysql 传统关系数据库
- 安装scray_redis
    - pip install scrapy_redis
    - scrapy-redis.readthedocs.org
    
   
   
              
                                       
            